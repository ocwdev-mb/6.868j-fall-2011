---
content_type: page
description: This section contains an optional homework assignment for the course.
learning_resource_types:
- Assignments
ocw_type: CourseSection
parent_title: Assignments
parent_type: CourseSection
parent_uid: 4d182964-96d8-6a26-f8f1-e97fd030b967
title: Homework 3
uid: b9b30550-fa5b-1e51-c0e5-3cf49d633f2f
---

These questions are on chapters 4–9 on the Emotion Machine.

**Problem 1.** One critical component of problem solving is having a good representation of the problem. This entails representing the relevant objects, properties and relations at the right level of detail{{< sup "1" >}}. You'll also need to represent a goal state in order to determine progress, and possibly some anti-goal states that you wish to avoid{{< sup "2" >}}.

When knowledge is shared among members of a community, it is called **commonsense knowledge** and it consists of generalizations that are "true" by default. Because there are often exceptions to generalizations, it is indirect to talk about knowledge being _true_ or _false_ without ﬁrst explaining the problem solving context: other assumptions (commonsense knowledge) that are left implicit. This problem is pervasive in natural language, which is the form of knowledge you will be dealing with for this problem. Instead of using the vague notion of truth, we can characterize knowledge as _useful_ for solving a particular problem or class of problems, it is consistent within its local context, but not necessarily consistent with all of the agent's knowledge.

The goal of this assignment is to get you thinking about knowledge: how it can be used, represented, and organized within a resourceful commonsense reasoning architecture. Good answers will show ingenuity and comprehension of a detailed problem solving architecture.

Take a look at the ﬁrst 500 entries in the [OpenMind](http://p2pfoundation.net/Open_Mind_Commons) project and **pick three assertions** from the corpus. These statements all depend on other hidden common sense assumptions that you should try to unpack. For each, answer the following questions:

1.  What are some problems/goals that this assertion would be useful for solving?
2.  What are some problems/goals that appear, on the surface{{< sup "3" >}}, to be related to the knowledge but are not?
3.  Describe a computational (procedural or representational) mechanism for distinguishing between using this knowledge for a problem in (1) from using it in a irrelevant problem in (2).
4.  Imagine that you have acquired new information (ie by perception) that is related to this assertion that supports or disagrees with your assertion. Write this new knowledge down.
5.  Combine your two assertions to derive a new piece of useful knowledge (by induction, deduction, abduction, or analogy{{< sup "4" >}}) or other information (eg a contradiction).
6.  Combine your two assertions to derive a useless/absurd conclusion.
7.  Describe a way to avoid drawing the useless/absurd conclusion (6). For example use and describe a reﬂective critic, structural knowledge, or an invention of your own.

**Problem 2.** For each of the layers of Model–6, give a concise description of the layer's function in your own words along with a supporting example.

**Problem 3. Self-Models**

*   What are some reasons why you would need to switch between many small models rather than just using one complete model for all problems?
*   What are the advantages of having a self model? What are some disadvantages?
*   Given an example of some software or hardware artifact that uses self models to some extent.

**Problem 4.** In the section on Learned Reactions, Minsky describes how IF DO rules alone are not enough because there are always exceptions for each rule, which would make the assertion false. What beneﬁts does adding THEN to IF DO rules provide?

{{< sup "1" >}}If the description is too vague/ambiguous, it may match too many items; however, if it is too speciﬁc or precise, it may never match anything!

{{< sup "2" >}}See: Minsky, Marvin. "[Negative Expertise](http://web.media.mit.edu/~minsky/papers/NegExp.mss.txt)." _International Journal of Expert Systems_ 7 no. 1 (1994): 13-19.

{{< sup "3" >}}For example, they share some of the same objects, properties or relations.

{{< sup "4" >}}If you don't understand these terms, see: Sowa, John. ["The Challenge of Knowledge Soup" (PDF)](http://www.jfsowa.com/pubs/challenge.pdf). _jfsowa.com_. Explain any additional missing background knowledge that was relevant to your inference.

{{% resource_link 4d182964-96d8-6a26-f8f1-e97fd030b967 "« Back to assignments" %}}